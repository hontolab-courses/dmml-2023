{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91c2666-e4e1-469a-b960-b7e9350daa90",
   "metadata": {},
   "source": [
    "# 第8回: ニューラルネットワーク入門"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6389dfce-aacf-43f7-9586-36620d5c6522",
   "metadata": {},
   "source": [
    "このHands-onでは，下記のデータを用いてニューラルネットワークによる分類器構築と勾配法の動作確認を体験します．\n",
    "* ファッション商品写真の画像データ\n",
    "* 人工的に作られたある2次元データ\n",
    "\n",
    "Hands-onに先立って，必要なライブラリを読み込んでおきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5c58f-8fb8-4334-a73c-24c37c7a45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワーク構築の基礎となるPyTorchライブラリ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorchが提供しているデータセットおよびそれを扱うライブラリ\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 乱数を扱うライブラリ\n",
    "import random\n",
    "\n",
    "# グラフ描画ライブラリ\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# グラフ描画のためのおまじない\n",
    "sns.set()\n",
    "sns.set_style('ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a91f7-64d4-4656-9f9a-fbd1fae61e78",
   "metadata": {},
   "source": [
    "---\n",
    "## 例題1: Fashion-MNIST - ファッション商品写真の画像データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c205b58-f31a-4dea-9d54-630c697a99d1",
   "metadata": {},
   "source": [
    "例題1では，ファッション商品写真の分類を行うニューラルネットワークの構築を体験します．用いるデータは，手書き数字画像データセットのファッション商品版である[Fashion-MNISTデータセット](https://github.com/zalandoresearch/fashion-mnist)です．Fashion-MNISTは，下記10種類のファッション商品に関する画像とそのラベルが収められたデータセットです：\n",
    "* Tシャツ/トップス（ラベル0）\n",
    "* ズボン（ラベル1）\n",
    "* セーター（ラベル2）\n",
    "* ドレス（ラベル3）\n",
    "* コート（ラベル4）\n",
    "* サンダル（ラベル5）\n",
    "* シャツ（ラベル6）\n",
    "* スニーカー（ラベル7）\n",
    "* バッグ（ラベル8）\n",
    "* ブーツ（ラベル9）\n",
    "\n",
    "データセットは，6万枚の学習用データと1万枚の評価用データで構成されています（それぞれデータは画像とラベルのペアで構成）．また，各画像は28x28ピクセルのグレースケール画像（0-255の数値で濃淡を表現）となっています．以下は，[公式サイト](https://github.com/zalandoresearch/fashion-mnist)に掲載されているFashion-MNIST中のファッション商品画像の例です．\n",
    "\n",
    "![Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)\n",
    "\n",
    "今回はこのデータセットを使って，ファッション商品画像データから商品ラベルを推定するニューラルネットワークを構築してみましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593d135-57d6-4f0a-8289-1ecb4f8b29ac",
   "metadata": {},
   "source": [
    "### データセットの準備\n",
    "まずはデータセットを準備します．以下のコードを実行してください．学習用データと評価用データが`data`ディレクトリにダウンロードされます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089e6b7-e5b4-418a-b921-9be3f97d35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データ\n",
    "dataset_train = torchvision.datasets.FashionMNIST(\n",
    "                    root='data',\n",
    "                    train=True,\n",
    "                    transform=transforms.ToTensor(),\n",
    "                    download=True)\n",
    "\n",
    "# 評価用データ\n",
    "dataset_test = torchvision.datasets.FashionMNIST(\n",
    "                    root='data',\n",
    "                    train=False,\n",
    "                    transform=transforms.ToTensor(),\n",
    "                    download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fcddf-0444-4e92-a3eb-386fbb22c8f2",
   "metadata": {},
   "source": [
    "今回用いるデータセットはあらかじめ学習用データと評価用データが分かれているので，自分で評価データを作成することなく分類器の性能評価が行えます．そのため今回は\n",
    "* 学習用データ全体をニューラルネットワークの学習，\n",
    "* 評価用データ全体をニューラルネットワークの評価\n",
    "\n",
    "に使うことにしましょう．\n",
    "\n",
    "さて，データセット中では，商品ラベルは数値で表現されています．推定結果を分かりやすくするために，以下を実行して，数値ラベルと商品ラベルを対応づけをするための準備をしておきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f39c7-e895-4999-8555-139c84ad42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "     0: \"T-shirt/Top\", # Tシャツ/トップス\n",
    "     1: \"Trouser\",     # ズボン\n",
    "     2: \"Pullover\",    # セーター\n",
    "     3: \"Dress\",       # ドレス\n",
    "     4: \"Coat\",        # コート\n",
    "     5: \"Sandal\",      # サンダル\n",
    "     6: \"Shirt\",       # シャツ\n",
    "     7: \"Sneaker\",     # スニーカー\n",
    "     8: \"Bag\",         # バッグ\n",
    "     9: \"Ankle Boot\"   # ブーツ\n",
    "}\n",
    "\n",
    "# クラス数（ラベル数）\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a774a8-9cac-499c-b8fc-e6b5e01e58e9",
   "metadata": {},
   "source": [
    "#### データセットを小分けにして取り出せるようにする\n",
    "深層学習を含め，ニューラルネットワークの学習では大規模なデータセットを扱います．今回はそれほど大きなデータセットではありませんが，それでも学習用データの画像数は6万枚あります．通常ニューラルネットワークの学習を行う場合には，データセット全体を一気にネットワークに投入することはせず，データセットを小分けして学習を行います．こうすることで，処理を軽くしたり計算を効率化することができます．\n",
    "\n",
    "今回用いるPyTorchライブラリには，データを小分けにしながら学習を行うための便利クラス`DataLoader`が用意されています．これを使うことにしましょう．今回の学習では，データを256個ずつに小分けしながら処理を行うことにします．以下のコードを実行し，小分け処理の準備を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bea93-e5c9-46fc-a0eb-ce30740d23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小分け処理をするデータのサイズを指定\n",
    "batch_size = 256\n",
    "\n",
    "# 学習用のデータ小分け機能の定義\n",
    "loader_train = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# 評価用のデータ小分け機能の定義\n",
    "loader_test = torch.utils.data.DataLoader(dataset=dataset_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a963355-b9bc-4dbe-94a0-885a46b37412",
   "metadata": {},
   "source": [
    "### ニューラルネットワークの定義\n",
    "ここがニューラルネットワークの本丸です．ラベル推定を行うニューラルネットワークを定義しましょう．\n",
    "\n",
    "今回用いる[PyTorch](https://pytorch.org)は，[TensorFlow](https://www.tensorflow.org)と双璧をなす深層学習用のライブラリです．非常に柔軟にニューラルネットワークを定義・学習・評価することができます．PyTorchを使えば「ザ・深層学習」なニューラルネットワークも簡単に構築できます．\n",
    "\n",
    "今回の例題では，（入力を除いて）3層しかないシンプルで古典的な多層パーセプトロンを構築します．具体は以下の通りです：\n",
    "* 0層目：入力層．28x28ピクセルの入力に対応して，784次元のベクトルの入力を受け付ける\n",
    "* 1層目：隠れ層1．ユニット数（ノード数）は1024\n",
    "* 2層目：隠れ層2．ユニット数は512\n",
    "* 3層目：出力層．ユニット数は10（ラベル数に対応）\n",
    "\n",
    "図にすると，以下になります．\n",
    "\n",
    "![](https://raw.githubusercontent.com/hontolab-courses/dmml-2023/main/network.png)\n",
    "\n",
    "また，1層目と2層目の活性化関数には**シグモイド関数**を，出力層である3層目には**ソフトマックス関数**を用いることにします．これらの要件を満たす多層パーセプトロン（Multilayer Perceptron: MLP）は，以下のコードで表現できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5d276-3e0a-4b79-81dd-ac3f09f1064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1層目の入力は28x28次元，出力は1024次元（fc: fully connected）\n",
    "        self.fc1 = nn.Linear(28*28, 1024)\n",
    "        # 2層目の入力は1024次元，出力は512次元\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        # 3層目の入力は512次元，出力は10次元（分類ラベル数）\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    # 順伝播を定義\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f39f9f-1270-4f0a-be60-8c8ef09e7a9d",
   "metadata": {},
   "source": [
    "PyTorchではニューラルネットワークを定義するのにいくつかの書き方があるのですが，上記の書き方は\n",
    "* MLPという名前のニューラルネットワークをクラスとし，\n",
    "* その属性として3層のレイヤーがあることを定義．\n",
    "* forwardメソッドで，順伝播の流れを定義する\n",
    "\n",
    "という作法になっています．このネットワークの書き方は一見複雑そうに見えますが，他者が定義したネットワークを見たり，自分でネットワークを定義することを繰り返すことで自然と身に付いていきます．今の時点では，コードを見て何をしているのかが追えれば十分です．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d13026-01e0-43f0-92ac-76665dd4c9ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 学習\n",
    "多層パーセプトロンの構造が定義できましたので，ニューラルネットワークの学習・評価を行います．学習と評価のプロセスはどんなニューラルネットワークを用いたとしても，大体次の内容をコードで書くことになります．\n",
    "\n",
    "1. ニューラルネットワークのインスタンスを作る（初期化をする）\n",
    "2. 損失関数を定義する\n",
    "3. 最適化方法（勾配法の種類）を定義する\n",
    "4. 学習用データを使ってニューラルネットワークの学習を行う\n",
    "5. ステップ4で学習したニューラルネットワークの評価を行う\n",
    "6. ステップ4-5をエポック数（学習回数）分繰り返す\n",
    "\n",
    "以下，上の流れに沿ってコードを動かしてみましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c600b8-5040-4f00-9391-823b57b74f07",
   "metadata": {},
   "source": [
    "#### Step 1: ニューラルネットワークのインスタンスの生成\n",
    "まず，先ほど定義した多層パーセプトロンのインスタンスを作ります．具体的には，定義したニューラルネットワークのパラメータにランダムな値を割り当てたものを生成します．実践では，このタイミングで**GPUを使って計算するか，CPUを使って計算するか**を指定します．\n",
    "\n",
    "ニューラルネットワークは非常に計算量が多いので，GPUの力を借りないと計算に時間がかかります．GPU環境がある場合は，以下のコードのように`device='cuda'`と指定することで，GPUを使った計算が可能となります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61dda2-d2fa-4e81-8e0f-d627a147cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算環境がApple Siliconを使用している場合，Metal を使うようにする\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    # 計算環境が NVIDIA GPUに対応していればGPUを使うようにする\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3404d-2f3d-47a6-9416-42f6a219d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算環境に応じてニューラルネットワークを初期化する\n",
    "model = MLP().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12cc3f-9dfd-46b0-9e08-c3c0319f3152",
   "metadata": {},
   "source": [
    "#### Step 2 & 3: 損失関数と最適化条件の設定\n",
    "予測値と正解とのズレを評価する「損失関数」と損失関数の微分最小化をするための「勾配法」の種類を定義しましょう．\n",
    "\n",
    "今回はファッション商品画像から商品のラベルを当てる「分類器」を作ることが目的なので，損失関数として**交差エントロピー**を用います．また，最適化のための勾配法としては，ベーシックな**確率的勾配降下法**を使うことにしましょう．以下のコードを実行してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23bdb9-9468-4714-92be-841383f8fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数（交差エントロピー）の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法（確率的勾配降下法）を設定（学習率は0.01を指定）\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b40ed2-57ec-476b-8507-65da6e95217c",
   "metadata": {},
   "source": [
    "#### Step 4-6: 学習と評価 \n",
    "いよいよニューラルネットワークの学習と評価を行います．\n",
    "\n",
    "ニューラルネットワークの学習では，\n",
    "1. 用意された学習データを用いて「学習」を行い，\n",
    "2. 評価データを用いて学習されたネットワークを「評価」する\n",
    "\n",
    "という作業をエポック数回だけ繰り返すことになります．各エポックでは同じデータ，同じ方法を用いてネットワークの「学習」と「評価」を行います．そのため，「学習」と「評価」の手続きをあらかじめ定義しておいた方が楽です．それら手続きをコード化しましょう．\n",
    "\n",
    "以下は「学習フェーズ」のコードです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048379f-99bb-4f08-8254-8616de1ced11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataloader, device):\n",
    "    # ニューラルネットワークモデルを「学習」フェーズに設定しておく\n",
    "    model.train()\n",
    "    \n",
    "    # 損失関数の値を保存する変数\n",
    "    loss = 0\n",
    "    \n",
    "    # データ全体を小分け（バッチ）にして学習を行う\n",
    "    for batch_num, (images, labels) in enumerate(dataloader):\n",
    "        # 画像データとラベルデータをGPUに載せられたら載せる\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # 勾配をリセット\n",
    "        optimizer.zero_grad()\n",
    "        # 順伝播\n",
    "        outputs = model(images)\n",
    "        # lossを計算（CrossEntropyLossの内部でSoftmaxが実装されているので，Softmax関数をかまさなくてOK）\n",
    "        _loss = criterion(outputs, labels)\n",
    "        # 誤差逆伝播\n",
    "        _loss.backward()\n",
    "        # 重みの更新\n",
    "        optimizer.step()\n",
    "        # lossのミニバッチ分を加算\n",
    "        loss += _loss.item()\n",
    "   \n",
    "    # 損失の平均\n",
    "    loss = loss / len(dataloader.dataset)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd2440c-c8b3-434d-9e3e-e6489091e78b",
   "metadata": {},
   "source": [
    "複雑に見えるかもしれませんが，上記コードがやっていることは\n",
    "1. データを小分けにする\n",
    "2. 各小分けデータをニューラルネットワークに入力し損失関数の値を計算する\n",
    "3. 誤差を逆伝播させる\n",
    "4. 伝播された誤差と勾配情報から重みを更新する\n",
    "\n",
    "という操作を行っています．\n",
    "\n",
    "以下は「評価」フェーズのコードです．今回は分類問題を扱っているので，評価尺度として**Accuracy（精度）**を計算します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a86ff-cbf9-4e97-98fd-181b596b042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, optimizer, criterion, dataloader, device):\n",
    "    #  ニューラルネットワークモデルを「評価」フェーズに設定しておく\n",
    "    model.eval()\n",
    "    \n",
    "    # 損失関数の値や精度を保持しておく変数\n",
    "    loss = 0\n",
    "    total_num = 0\n",
    "    correct_label_num = 0\n",
    "    \n",
    "    # 評価するとき勾配を計算しないように加える\n",
    "    with torch.no_grad():\n",
    "        # データ全体を小分け（バッチ）にして評価を行う\n",
    "        for batch_num, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # 順伝播\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 評価用データに対する推論\n",
    "            labels_predicted = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # 推論の正答数を控えておく\n",
    "            correct_label_num += (labels_predicted == labels).sum()\n",
    "            total_num += len(labels)\n",
    "            \n",
    "            # 損失関数の値の計算\n",
    "            _loss = criterion(outputs, labels)\n",
    "            loss += _loss.item()            \n",
    "        \n",
    "        # 損失関数の値のまとめ（平均値）\n",
    "        loss = loss / len(dataloader.dataset)\n",
    "        # 精度の計算\n",
    "        accuracy = correct_label_num / total_num\n",
    "    return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fbc3c-4c5a-4ff4-a032-2b466d29b243",
   "metadata": {},
   "source": [
    "学習と評価を行うための準備が整いました．エポック数回だけ学習と評価を行ってみましょう．今回はエポック数を50に設定します．つまり，同じデータを使ってニューラルネットワークの学習と評価を50回繰り返します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2521776-57d9-4f36-b6f2-e3bce529dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック数の設定\n",
    "num_epochs = 50\n",
    "\n",
    "# 学習用データに対する推論の損失関数の値と評価データに対するそれを保持する変数\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "\n",
    "# エポック回数分，学習と評価を行う\n",
    "for epoch in range(num_epochs):\n",
    "    # 学習フェーズ\n",
    "    loss_train = train(model, optimizer, criterion, loader_train, device)\n",
    "    \n",
    "    # 評価フェーズ\n",
    "    loss_test, accuracy_test = evaluate(model, optimizer, criterion, loader_test, device)\n",
    "\n",
    "    loss_train_list.append(loss_train)\n",
    "    loss_test_list.append(loss_test)\n",
    "    \n",
    "    # 途中経過の表示\n",
    "    if epoch % 10 == 0 or epoch == 49:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss_train : {loss_train:.4f}, Loss_test : {loss_test:.4f}, Accuracy_test : {accuracy_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c1c6d-46df-4332-8fdc-5631a1c9332a",
   "metadata": {},
   "source": [
    "上記コードを走らせると，じわりじわりと学習が進みます．ニューラルネットワークの学習は非常に時間がかかるため，しばらく計算機を放置しておきましょう．途中，学習の状況が表示されますが，`loss_train`（学習データに対する損失関数値）と`loss_test`（評価データに対する損失関数値）の双方が順調に下がっていれば問題ありません．\n",
    "\n",
    "以前の講義でも話したように，学習回数が多すぎると過学習する恐れがあります．学習データに対する損失関数値が下がり続けている一方，評価データに対する損失関数値が上がり始めた場合は過学習が起こり始めた可能性があります．もし過学習が起き始めたら，そのタイミングで学習を終了させるのが良いでしょう．\n",
    "\n",
    "今回はエポック数が50あたりで学習を終了させましょう．エポック数が50の時の**Accuracy**は72−75%程度といったところでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01df7d-a54d-413a-af8c-b184d2e39b38",
   "metadata": {},
   "source": [
    "### 深層学習をやってみる\n",
    "講義ではやっていませんが，Fashion-MNISTの分類問題に深層学習でアタックしてみましょう．ここでは，画像系の深層学習でよく用いられる[畳み込みニューラルネットワークモデル](https://ja.wikipedia.org/wiki/畳み込みニューラルネットワーク)を使ってみます．詳細は説明しませんので，興味のない人はコードの実行結果だけ見て，上で行ったシンプルな3層の多層パーセプトロンの結果と比較してみてください．\n",
    "\n",
    "以下，今回用いる畳み込みニューラルネットワークの定義と学習・評価の実行結果です．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc90323-9f4d-4904-a4d8-6f32dc5c1f4f",
   "metadata": {},
   "source": [
    "#### 畳み込みニューラルネットワークの定義（一例）\n",
    "ネットワークとしては，畳み込み→ReLU→Max Pooling→畳み込み→ReLU→Max Pooling→アフィン変換→ReLU→アフィン変換という操作をしています（途中，ドロップアウトという一定の確率でユニットを無視するテクニックも使っています）．\n",
    "\n",
    "勾配法としては，深層学習ではよく用いられる[Adam](https://ja.wikipedia.org/wiki/確率的勾配降下法#Adam)という最適化手法を用いています．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdaee06-949d-4ddc-afe3-5728fc0a4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(in_features=64*7*7, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv 1\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # conv 2\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # fc layer\n",
    "        x = x.reshape(-1, 64*7*7)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = F.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        # output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d7eb6-60b5-4917-aef1-276397d35bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN().to(device)\n",
    "\n",
    "# 損失関数（交差エントロピー）の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法を設定（Adam）\n",
    "optimizer = optim.Adam(cnn_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea996d7-5014-4a2b-8e3c-7e4f8ca08745",
   "metadata": {},
   "source": [
    "#### 学習と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f2633-ab50-45e2-9474-6feffd2220ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用のデータ小分け機能の定義\n",
    "loader_train = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# 評価用のデータ小分け機能の定義\n",
    "loader_test = torch.utils.data.DataLoader(dataset=dataset_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# エポック数の設定\n",
    "num_epochs = 50\n",
    "\n",
    "# 学習用データに対する推論の損失関数の値と評価データに対するそれを保持する変数\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "\n",
    "# エポック回数分，学習と評価を行う\n",
    "for epoch in range(num_epochs):\n",
    "    # 学習フェーズ\n",
    "    loss_train = train(cnn_model, optimizer, criterion, loader_train, device)\n",
    "    \n",
    "    # 評価フェーズ\n",
    "    loss_test, accuracy_test = evaluate(cnn_model, optimizer, criterion, loader_test, device)\n",
    "\n",
    "    loss_train_list.append(loss_train)\n",
    "    loss_test_list.append(loss_test)\n",
    "    \n",
    "    # 途中経過の表示\n",
    "    if epoch % 10 == 0 or epoch == 49:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss_train : {loss_train:.4f}, Loss_test : {loss_test:.4f}, Accuracy_test : {accuracy_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93976d2-e225-4495-8ff9-cad3c60d427c",
   "metadata": {},
   "source": [
    "多層パーセプトロンよりも随分と時間がかかりますが，効果は抜群です．畳み込みニューラルネットワークモデルが最適化手法Adamによって効率よく学習されました．エポック数50における**Accuracy**が90%程度ですので，大幅に性能が改善されました．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87763994-efcc-446a-a12b-0065883f755d",
   "metadata": {},
   "source": [
    "#### 推論テスト\n",
    "せっかくなので，学習させた畳み込みニューラルネットワークで未知のファッション商品画像のラベル推定をやってみましょう．\n",
    "以下のコードでは評価用データからランダムにデータを1つ取り出し，それに対して分類器による推論を行っています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242a74c-e500-4983-8618-e12db50b5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetからサンプルを一つ取り出す\n",
    "image, label = random.choice(dataset_test)\n",
    "\n",
    "# 学習済みモデルを使ってラベルを推論\n",
    "# 下記コードでは，畳み込みニューラルネットワークの出力値が最も高いラベルを取得している\n",
    "label_predicted = torch.argmax(model(image.to(device).reshape(-1, 1, 28, 28)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.reshape(28, 28), cmap=\"gray\")\n",
    "fig.set_size_inches(2, 2)\n",
    "plt.show()\n",
    "print(\"True label:\", id2label[label])\n",
    "print(\"Predicted label:\", id2label[int(label_predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f878cf9-374f-4000-8d7b-d4b0596e0e6e",
   "metadata": {},
   "source": [
    "何回か推論を行ってみてください．大体当たっているのではないでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bcf2f-4401-410d-8f7a-7330cea438f8",
   "metadata": {},
   "source": [
    "---\n",
    "## 例題2: 勾配法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250a0a2-d566-43f5-9c17-598ea8928a11",
   "metadata": {},
   "source": [
    "最後に，機械学習の基礎となる技術「勾配法」を体験してみましょう．ニューラルネットワークの損失関数は複雑なので，それを対象として勾配法が関数の微分最小化をどのように行うかを体感するのは難しいかもしれません．そこで，この例題では人工的に生成した2次元データを用いて，勾配法による2次関数の最小化問題を体験します．\n",
    "\n",
    "まずは以下のコードを実行して，例題で用いるデータを読み込んでください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcee4c-337e-4480-a806-f4f0ac802157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表データを扱うライブラリ\n",
    "import pandas as pd\n",
    "\n",
    "# データを読み込む\n",
    "data_url = \"https://raw.githubusercontent.com/hontolab-courses/dmml-2023/main/dataset/sgd-data.tsv\"\n",
    "df = pd.read_table(data_url, header=0, sep=\"\\t\")\n",
    "\n",
    "# 末尾10件のデータを表示\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8ae19-9abe-485d-898b-85cde337b206",
   "metadata": {},
   "source": [
    "このデータセットには，上記のような2次元のデータが1000個収められています．以下のコードを実行して，データの分布を確認してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7b65e-ea3a-46d8-926e-f3db2ac4029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "sns.scatterplot(x=df[\"x\"], y=df[\"y\"], linewidth=0, s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4a266-64c9-442b-9850-bf78309da707",
   "metadata": {},
   "source": [
    "散布図を見る限り，データは直線的に分布しているようです．そこで，「このデータセットは直線\n",
    "$$\n",
    "\\begin{align}\n",
    "y=&f(x)\\\\\n",
    "　=&wx - 1.59\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "に従って発生したもの」と仮定し，直線のパラメータ$w$（傾き）を勾配法を用いて推定することにしましょう（なぜ切片が-1.59かというと，データ作成者である山本はこの直線が何かを知っているからです…）．\n",
    "\n",
    "さて，直線のパラメータはどのように求めるか．わたしたちはこの直線に従って発生したと思われるデータ集合$D=\\{(x_1, y_1),...,(x_{1000}, y_{1000})\\}$を持っています．ここでは，予想される直線（$y=f(x)$）から推測される$y$と実際の$y$の値の2乗誤差の総和を損失関数と見なし，その損失関数を最小化するパラメータ$w$を見つける戦略を採りましょう（いわゆる[最小二乗法](https://ja.wikipedia.org/wiki/最小二乗法)）．つまり，以下の関数$L$を最小化するパラメータ$w$を見つけることにします（$L$は$w$に関する2次関数）．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L=& \\sum_{i=1}^{1000}(y_i - f(x_i))^2 \\\\\n",
    " =& \\sum_{i=1}^{1000}(y_i - wx_i + 1.59)^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01df76-b6f6-4e1e-baab-6a6ff9636237",
   "metadata": {},
   "source": [
    "計算機で関数を微分最小化するには，勾配法を用いるのが定番です．今回は勾配法の中でも**最急降下法**を用いることにします．関数$L(w)$を最急降下法で微分最小化するには，適当な$w^{(0)}$からスタートして，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\frac{dL}{dw}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "のルールを繰り返し適応しながら$w$を更新していきます．ここで関数$L(w)$の$w$に関する導関数（勾配）は\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{dw} =& \\sum_{i=1}^{1000}2(-x_i)(y_i - wx_i + 1.59)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "となるので，パラメータ$w$の更新式は\n",
    "$$\n",
    "\\begin{align}\n",
    "w^{k+1} = w^{k} - \\alpha \\sum_{i=1}^{1000}2x_i(wx_i - 1.59 - y_i)\n",
    "\\end{align}\n",
    "$$\n",
    "となります．\n",
    "\n",
    "ということで，この更新式を使って最急降下法による関数の微分最小化および最適なパラメータの推定を行ってみましょう．最急降下法ではパラメータの更新の細かさを決める学習率$\\alpha$を設定する必要があります．今回は$\\alpha=10^{-4}$としましょう．また，パラメータ更新の終了条件は，\n",
    "* パラメータ$w$の更新回数が10000回になったとき\n",
    "* パラメータ$w$の更新量の絶対値が$10^{-8}$未満になったとき（つまりパラメータがほとんど変化しなくなったとき）\n",
    "\n",
    "としましょう．前置きが長くなりましたが，以上のことを踏まえて，コードを書きます．まずは，関数$L(w)$の$w$に関する勾配をコードで定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d85d0e-cefe-45fd-9b06-2625ba6924a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x_list, y_list, w):\n",
    "    grad = 0\n",
    "    \n",
    "    # データを1個ずつ処理\n",
    "    for x, y in zip(x_list, y_list):\n",
    "        grad += 2 * x * (w * x  - 1.59 - y)\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4506a6d-3031-4740-8cb5-8570c8a8aa8a",
   "metadata": {},
   "source": [
    "勾配の式が定義できたので，最急降下法のコードを実装します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75291f-1c2d-4480-9fab-26aa76262de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_list, y_list, alpha=0.0001, delta_threshold=1e-8, epoch_num=10000):\n",
    "    \"\"\" alpha: 学習率，\n",
    "        delta_treshold: パラメータの更新量の絶対値に対する閾値\n",
    "        epoch_num: 更新回数の上限\n",
    "    \"\"\"\n",
    "    \n",
    "    # パラメータをランダムに初期化\n",
    "    w = random.random()\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        # パラメータの更新式\n",
    "        w_new = w - alpha * gradient(x_list, y_list, w)\n",
    "\n",
    "        # パラメータの更新量の絶対値\n",
    "        w_delta = abs(w_new - w)\n",
    "        if w_delta < delta_threshold:\n",
    "            # パラメータがほぼ変化しなくなったら\n",
    "            return w_new\n",
    "        else:\n",
    "            # まだ変化する余地があるなら，引き続きパラメータを更新する\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"更新回数 {epoch}回目\\t\", w)\n",
    "            w = w_new\n",
    "            \n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab7891f-2ffc-4d5f-b00a-cc99843cdcae",
   "metadata": {},
   "source": [
    "準備が整ったので，最急降下法を実行してみましょう．以下のコードを走らせてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912ac1c-d30f-42d2-9d4c-c738fd5e79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット\n",
    "x_list = df[\"x\"]\n",
    "y_list = df[\"y\"]\n",
    "\n",
    "# 最急降下法の実行\n",
    "w = gradient_descent(x_list, y_list, 0.0001, 1e-8, 10000)\n",
    "\n",
    "print(\"\\nOptimized w =\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153159bd-60af-4f40-8172-2d6e740bd4db",
   "metadata": {},
   "source": [
    "$w$の値として，3.14付近の値が推定されたのではないでしょうか．実は今回用いたデータセットは，$y=3.14x-1.59$という直線から発生させたものでした．直線の傾きは3.14ですので，推定されたパラメータ$w$はおおよそ当たっていたことになります．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
