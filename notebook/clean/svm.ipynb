{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec84b77a-881c-4f4d-846f-51e33e1e5cea",
   "metadata": {},
   "source": [
    "# 第5回: サポートベクターマシン"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91921de-ab3e-4bfd-b18a-7181f4c7b6d0",
   "metadata": {},
   "source": [
    "このHands-onでは下記2種類のデータを用いて，サポートベクターマシンについて体験します．\n",
    "\n",
    "* 手書き数字画像（第4回のHands-onでも用いたデータ）\n",
    "* 人工的に作られたある2次元データ\n",
    "\n",
    "Hands-onに先立って，必要なライブラリを読み込んでおきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7deeab7-4ace-40dd-bdaa-5d5d2b3a4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表形式のデータを操作するためのライブラリ\n",
    "import pandas as pd\n",
    "\n",
    "# 行列計算をおこなうためのライブラリ\n",
    "import numpy as np\n",
    "\n",
    "# データセット\n",
    "from sklearn import datasets\n",
    "\n",
    "# K近傍法を実行するためのクラス\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# サポートベクターマシンを実行するためのクラス\n",
    "from sklearn import svm\n",
    "\n",
    "# 交差検証を行うためのクラス\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# グラフ描画ライブラリ\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b8f34-9c85-412b-8d66-538af66a08f5",
   "metadata": {},
   "source": [
    "---\n",
    "## 例題1: MNIST手書き数字データ\n",
    "\n",
    "例題1では，K近傍法のHands-onで用いた[MNISTデータセット](http://yann.lecun.com/exdb/mnist/)を用いて，手書き数字の識別するサポートベクターマシン分類器を構築します．データセットの詳細は，[前回のHands-on](https://nbviewer.org/github/hontolab-courses/dmml-2021/blob/main/notebook/knn-and-ml-ops.ipynb)を参照してください．\n",
    "\n",
    "それでは，以下のコードを実行してMNISTデータセットを読み込みましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ca3da-ec8c-409c-ae5e-f5917d49f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = datasets.load_digits()\n",
    "X = mnist_dataset.data\n",
    "y = mnist_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1e63c-f0ff-4053-9de9-1f63c7fa81a7",
   "metadata": {},
   "source": [
    "前回の「K近傍法」の授業では，機械学習を実際に行う際には\n",
    "* 前処理\n",
    "* 学習･評価の流れ\n",
    "* 評価手法\n",
    "\n",
    "に関するお作法があることを学びました．K近傍法や機械学習のお作法の復習のために，まずはK近傍法を使ってMNISTの分類器の構築・評価を行ってみましょう．今回は5分割交差検証を行い，評価手法にはマクロ精度（Balanced Accuracy）を用いることにします．\n",
    "\n",
    "下記コードはMNISTデータセットにK近傍法を適用する例です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab00591-7572-4aba-a33f-3aeef52a1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5分割「層別化」交差検証の準備（乱数を固定）\n",
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "\n",
    "# 評価指標（マクロ精度）\n",
    "score_funcs = ['balanced_accuracy']\n",
    "\n",
    "# K近傍法のモデルの定義（K=5で設定）\n",
    "K = 5\n",
    "knn_model = KNeighborsClassifier(n_neighbors=K, metric=\"euclidean\")\n",
    "\n",
    "# 交差検証をしながら分類器を構築・評価\n",
    "scores = cross_validate(knn_model, X, y, cv=k_fold, scoring=score_funcs)\n",
    "\n",
    "# 交差検証の評価スコアを平均としてまとめる\n",
    "print(\"Balanced accuracy: \", np.mean(scores['test_balanced_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b396d7-89b2-4216-ba23-910a164ecf1a",
   "metadata": {},
   "source": [
    "マクロ精度は99\\%程度になりました．K近傍法でも十分な分類性能を発揮しますね．次はサポートベクターマシンで手書き数字画像の分類器を構築・評価してみましょう．\n",
    "\n",
    "機械学習万能ライブラリである`sklearn`は，サポートベクターマシンに関するクラスも提供しています．`sklearn`ではサポートベクターマシンに関連するクラスとして\n",
    "* 分類問題を扱うための`svm.SVC`クラス\n",
    "* 回帰問題を扱うための`svm.SVR`クラス\n",
    "\n",
    "の2種類を提供しています．今回は分類問題を扱うので，`svm.SVC`クラスを用います．今回はカーネルトリックは使わず，単純な線形サポートベクターマシンを使ってみましょう．\n",
    "\n",
    "`sklearn`ライブラリを使えば，上のK近傍法コードを一行変えるだけでサポートベクターマシンによる分類器構の築・評価ができてしまいます．以下の修正したコードを確認・実行してみてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc74c9a-fcce-485b-b863-e0d027ec878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "score_funcs = ['balanced_accuracy']\n",
    "\n",
    "# 線形サポートベクターマシン（ここが修正点）\n",
    "svm_model = svm.SVC(kernel='linear')\n",
    "\n",
    "scores = cross_validate(svm_model, X, y, cv=k_fold, scoring=score_funcs)\n",
    "print(\"Balanced accuracy: \", np.mean(scores['test_balanced_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e2e6d3-a9c0-4e76-9e5e-5e4d73a57d14",
   "metadata": {},
   "source": [
    "サポートベクターマシンを用いたときのマクロ精度は98%程度になりました．MNISTデータセットの分類問題に関しては，サポートベクターマシンとK近傍法は同じくらいの性能を発揮することが分かりました．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717c584-6818-43a4-8e15-780c136bb7c1",
   "metadata": {},
   "source": [
    "---\n",
    "## 例題2: 線形分離不可能なデータに対するサポートベクターマシン"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ab12b-dbc0-4efb-8231-6fea7812d731",
   "metadata": {},
   "source": [
    "次の例題では，人工的な2次元データの分類問題を扱います．まずは，今回の例題で用いるデータを読み込みましょう．以下のコードを実行します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb6ab1-0a49-419b-af7b-51a2b67ca24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons\n",
    "moons = make_circles(n_samples=300, noise=0.1, factor=0.6, random_state=777)\n",
    "\n",
    "X = moons[0]\n",
    "y = moons[1]\n",
    "\n",
    "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47440699-0eb7-4b28-aa33-ffc7056687cf",
   "metadata": {},
   "source": [
    "今回用いるデータは，授業でも扱った「ドーナツ型」のデータ分布をしています．内側にオレンジ色の点（ラベル1）が，外側に青色の点（ラベル0）が分布しています．これらデータを用いて，データがラベル1かラベル0かを分類する分類器を構築してみましょう．\n",
    "\n",
    "まず，線形SVMを用いて分類器を構築・評価してみましょう．以下のように例題1のコードをほぼそのまま流用することで，線形SVMの学習・評価ができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce925d1-584b-419e-ba9c-b4ff11b88818",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "\n",
    "# 今回は評価指標として適合率（precision）と再現率（recall）を使用\n",
    "score_funcs = ['precision_macro', 'recall_macro']\n",
    "\n",
    "# 線形サポートベクターマシン（ここが修正点）\n",
    "svm_model = svm.SVC(kernel='linear')\n",
    "\n",
    "scores = cross_validate(svm_model, X, y, cv=k_fold, scoring=score_funcs)\n",
    "print(\"Precision: \", np.mean(scores['test_precision_macro']))\n",
    "print(\"Recall: \", np.mean(scores['test_recall_macro']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2b17c-1633-41b5-a9d8-545e8e623661",
   "metadata": {},
   "source": [
    "適合率も再現率も49%ですので，デタラメにラベルを答える場合とほぼ性能が分からないと言えるでしょう．授業でも説明しましたが，素のサポートベクターマシン（線形SVM）は，今回用いるデータセットのような**線形分離不可能**な分類問題にはうまく対応できません．このような場合は，データを高次元に写像することでうまく対応できる可能性があります．\n",
    "\n",
    "普通の流れならここでカーネルトリックを用いた非線形SVMを用いるのですが，少し寄り道をしましょう．手動でデータを高次元に写像し，そのデータに対して線形SVMを適用してみましょう．\n",
    "\n",
    "今回用いたデータは2次元データでした．ここで，1次元目の要素を$x_1$，2次元目の要素を$x_2$としたとき，$\\sqrt{x_1^2+x_2^2}$を3次元目の要素としてデータに追加しましょう．以下のコードを実行します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afea45-77cd-4696-a0ff-af94164444f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.insert(X, 2, np.sqrt(X[:, 0]**2+X[:, 1]**2), axis=1)\n",
    "\n",
    "# データの末尾10件のみ表示\n",
    "X_new[-10:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec45ff-a74b-4617-91a9-8a266132d1d4",
   "metadata": {},
   "source": [
    "3次元目の要素がデータに追加されました．\n",
    "新たに作った`X_new`を使って，再度サポートベクターマシンを適用してみましょう．以下のコードを実行します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f8166-4fca-4948-b8da-c290c9de02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "score_funcs = ['precision_macro', 'recall_macro']\n",
    "svm_model = svm.SVC(kernel='linear')\n",
    "\n",
    "# XをX_newに修正\n",
    "scores = cross_validate(svm_model, X_new, y, cv=k_fold, scoring=score_funcs)\n",
    "print(\"Precision: \", np.mean(scores['test_precision_macro']))\n",
    "print(\"Recall: \", np.mean(scores['test_recall_macro']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74a20b-f6ff-4098-8b68-15aff482cea2",
   "metadata": {},
   "source": [
    "適合率も再現率も大幅に向上しました．今回のデータは円の形に分布していたので，3時限目のデータとして円の半径に相当するような要素を追加しました．それが功を奏して分類器の性能が向上したわけです．しかし通常，このような都合の良いの高次元写像を地力で見つけるのは容易ではありません．\n",
    "\n",
    "こんなときに有用なのが**カーネルトリックを用いた非線形SVM**です．例題の締めとして，**RBFカーネルを用いた非線形SVM**による分類器の構築・評価を行ってみましょう．\n",
    "\n",
    "`sklearn`ライブラリを用いれば，非線形SVMも簡単に実行できてしまいます．今回はパラメータチューニングはせず，デフォルトの状態でRBFカーネルを用いてみましょう．以下のコードを実行して，（手動で高次元化していない）元のデータを用いた学習・評価を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2043209-9139-4026-b247-e74c861652d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "score_funcs = ['precision_macro', 'recall_macro']\n",
    "\n",
    "# カーネルにRBFカーネルを指定\n",
    "svm_model = svm.SVC(kernel='rbf')\n",
    "\n",
    "# X_newはXに戻した\n",
    "scores = cross_validate(svm_model, X, y, cv=k_fold, scoring=score_funcs)\n",
    "\n",
    "print(\"Precision: \", np.mean(scores['test_precision_macro']))\n",
    "print(\"Recall: \", np.mean(scores['test_recall_macro']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72f00c-5e7e-4a8c-bcfe-30af8431e141",
   "metadata": {},
   "source": [
    "データを手動で高次元空間に写像しなくても，RBFカーネルが良い具合に振る舞ってくれました．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
